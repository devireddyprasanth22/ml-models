{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Linear regression is a statistical technique that models the probability of events, given one or more independant variables. It is mainly used for binary classification tasks\n",
    "\n",
    "Unlike in linear regression, where the hypothetical function `y= wx + b` help with calculating unbound values, here we need to find either 0 or 1 (True and False)\n",
    "\n",
    "So the logistic regression model is as follows,\n",
    "\n",
    "$$\n",
    "y = sigmoid( wx + b )\n",
    "$$\n",
    "\n",
    "sigmoid is an activation function that is defined as,\n",
    "\n",
    "$$\n",
    "sigmoid(z) = \\frac{1}{1 + e^-z}\n",
    "$$\n",
    "\n",
    "Where `e` is the euler number which is equal 2.71828 and `z` is the dependant variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (loss)\n",
    "\n",
    "The cost function used for a logistic regression method is a convex function, whereas the least squared error method used in linear regression is non convex (refer to Notes for more details). A common function used for logistic regression is Binary Cross Entropy\n",
    "\n",
    "$$\n",
    "BCE = -\\frac{1}{m} \\sum_{i=1}^{m} ylog(\\hat{y}) + (1-y)log(1-\\hat{y})\n",
    "$$\n",
    "\n",
    "where $\\hat{y}$ is the predicted value from the logistic regression equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Gradient partials \n",
    "\n",
    "For weights ($x_i^T$ is the transposed x value at ith position),\n",
    "\n",
    "$$\n",
    "\\frac{\\partial BCE}{\\partial w_j} = \\frac{1}{m} (\\hat{y}_i - y_i) x_i^T\n",
    "$$\n",
    "\n",
    "For bias,\n",
    "$$\n",
    "\\frac{\\partial BCE}{\\partial b} = \\frac{1}{m} (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "\n",
    "The partials can then be used in the general gradient descent calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The implementation would involve a cost_function() for BCE, compute_gradients() for the partials, grad_desc() and a pred() for predictions and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets as ds\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)   \n",
    "bc = ds.load_breast_cancer() # 569 samples, 2 target classes, 30 features, can be found at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "X, y = bc.data, bc.target\n",
    "print(len(bc.feature_names))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def cost_function(self, y, y_pred):\n",
    "        epsilon = 1e-9 # the reason to add epsilon is to prevent any crashes when log(0) which can be inf\n",
    "        m = len(y)\n",
    "        y1 = y * np.log(y_pred + epsilon)\n",
    "        y2 = (1-y) * np.log(1-y_pred+epsilon)\n",
    "        return -(y1 + y2)/m\n",
    "    def compute_partials()\n",
    "    def train() # basically gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "**Why use a different loss function from linear regression?**\n",
    "\n",
    "If you try to use the linear regression's cost function in a logistic regression problem, you would end up with a non-convex function: a wierdly-shaped graph with no easy to find minimum global point. \n",
    "\n",
    "This strange outcome is due to the fact that in logistic regression we have the sigmoid function around, which is non-linear (i.e. not a line). The gradient descent algorithm might get stuck in a local minimum point. That's why we still need a neat convex function as we did for linear regression: a bowl-shaped function that eases the gradient descent function's work to converge to the optimal minimum point.\n",
    "\n",
    "*Source can be found [here](https://www.internalpointers.com/post/cost-function-logistic-regression)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

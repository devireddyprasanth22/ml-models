{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Linear regression is a statistical technique that models the probability of events, given one or more independant variables. It is mainly used for binary classification tasks\n",
    "\n",
    "Unlike in linear regression, where the hypothetical function `y= wx + b` help with calculating unbound values, here we need to find either 0 or 1 (True and False)\n",
    "\n",
    "So the logistic regression model is as follows,\n",
    "\n",
    "$$\n",
    "y = sigmoid( wx + b )\n",
    "$$\n",
    "\n",
    "sigmoid is an activation function that is defined as,\n",
    "\n",
    "$$\n",
    "sigmoid(z) = \\frac{1}{1 + e^-z}\n",
    "$$\n",
    "\n",
    "Where `e` is the euler number which is equal 2.71828 and `z` is the dependant variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (loss)\n",
    "\n",
    "The cost function used for a logistic regression method is a convex function, whereas the least squared error method used in linear regression is non convex (refer to Notes for more details). A common function used for logistic regression is Binary Cross Entropy\n",
    "\n",
    "$$\n",
    "BCE = -\\frac{1}{m} \\sum_{i=1}^{m} ylog(\\hat{y}) + (1-y)log(1-\\hat{y})\n",
    "$$\n",
    "\n",
    "where $\\hat{y}$ is the predicted value from the logistic regression equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Gradient partials \n",
    "\n",
    "For weights ($x_i^T$ is the transposed x value at ith position),\n",
    "\n",
    "$$\n",
    "\\frac{\\partial BCE}{\\partial w_j} = \\frac{1}{m} (\\hat{y}_i - y_i) x_i^T\n",
    "$$\n",
    "\n",
    "For bias,\n",
    "$$\n",
    "\\frac{\\partial BCE}{\\partial b} = \\frac{1}{m} (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n",
    "\n",
    "The partials can then be used in the general gradient descent calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The implementation would involve a cost_function() for BCE, compute_gradients() for the partials, grad_desc() and a pred() for predictions and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets as ds\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)   \n",
    "bc = ds.load_breast_cancer() # 569 samples, 2 target classes, 30 features, can be found at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "X, y = bc.data, bc.target\n",
    "print(len(bc.feature_names))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def cost_function(self, y, y_pred):\n",
    "        epsilon = 1e-9 # the reason to add epsilon is to prevent any crashes when log(0) which can be inf\n",
    "        m = len(y)\n",
    "        y1 = y * np.log(y_pred + epsilon)\n",
    "        y2 = (1-y) * np.log(1-y_pred+epsilon)\n",
    "        return -(y1 + y2)/m\n",
    "    \n",
    "    def compute_partials(self, X, y, y_pred):\n",
    "         error = y_pred - y\n",
    "         m = len(y)\n",
    "\n",
    "         dW = (1/m) * np.dot(X.T, error)\n",
    "         dB = (1/m) * np.sum(error)\n",
    "\n",
    "         return dW, dB\n",
    "    \n",
    "    def train(self, X, y): # basically gradient descent\n",
    "        loss_func = []\n",
    "        self.weights = np.zeros(X.shape[1])  # If X has shape (N, d), initialize weights as a vector of size d which is features\n",
    "        self.bias = 0  \n",
    "        for epoch in range(self.epochs):\n",
    "            y_hat = np.dot(X, self.weights) + self.bias # get y_pred\n",
    "            y_pred = self.sigmoid(y_hat)\n",
    "            loss = self.cost_function(y, y_pred) # get cost\n",
    "            loss_func.append(loss)\n",
    "            dW, dB = self.compute_partials(X, y, y_pred)  # compute partials\n",
    "\n",
    "             # use partials to update weights and biases\n",
    "            self.weights = self.weights - self.learning_rate * dW\n",
    "            self.bias = self.bias - self.learning_rate * dB\n",
    "\n",
    "            if epoch%2==0:\n",
    "                print(f\"Epoch {epoch}/{self.epochs}, Loss: {loss[0]}\")\n",
    "        return self.weights, self.bias, loss_func\n",
    "    def predict(self, X):\n",
    "        y_hat = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(y_hat)\n",
    "        y_pred_out = [1 if i > 0 else 0 for i in y_pred]\n",
    "        return np.array(y_pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000, Loss: 0.0015234003924394404\n",
      "Epoch 2/1000, Loss: 0.045545639202080025\n",
      "Epoch 4/1000, Loss: 0.045545639202080025\n",
      "Epoch 6/1000, Loss: -2.1978023785502656e-12\n",
      "Epoch 8/1000, Loss: -2.1978023785502656e-12\n",
      "Epoch 10/1000, Loss: -2.1978023785502656e-12\n",
      "Epoch 12/1000, Loss: -2.1978023785502656e-12\n",
      "Epoch 14/1000, Loss: 0.045545639202080025\n",
      "Epoch 16/1000, Loss: 0.045545639202080025\n",
      "Epoch 18/1000, Loss: -2.1978023785502656e-12\n",
      "Epoch 20/1000, Loss: -2.1978023785502656e-12\n",
      "Epoch 22/1000, Loss: 0.045545639202080025\n",
      "Epoch 24/1000, Loss: 0.045545639202080025\n",
      "Epoch 26/1000, Loss: 0.045545639202080025\n",
      "Epoch 28/1000, Loss: 0.045545639202080025\n",
      "Epoch 30/1000, Loss: 0.045545639202080025\n",
      "Epoch 32/1000, Loss: 0.045545639202080025\n",
      "Epoch 34/1000, Loss: 0.045545639202080025\n",
      "Epoch 36/1000, Loss: 0.045545639202080025\n",
      "Epoch 38/1000, Loss: 0.045545639202080025\n",
      "Epoch 40/1000, Loss: 0.045545639202080025\n",
      "Epoch 42/1000, Loss: 0.045545639202080025\n",
      "Epoch 44/1000, Loss: 0.045545639202080025\n",
      "Epoch 46/1000, Loss: 0.045545639202080025\n",
      "Epoch 48/1000, Loss: 0.045545639202080025\n",
      "Epoch 50/1000, Loss: 0.045545639202080025\n",
      "Epoch 52/1000, Loss: 0.045545639202080025\n",
      "Epoch 54/1000, Loss: 0.045545639202080025\n",
      "Epoch 56/1000, Loss: 0.045545639202080025\n",
      "Epoch 58/1000, Loss: 0.045545639202080025\n",
      "Epoch 60/1000, Loss: 0.045545639202080025\n",
      "Epoch 62/1000, Loss: 0.045545639202080025\n",
      "Epoch 64/1000, Loss: 0.045545639202080025\n",
      "Epoch 66/1000, Loss: 0.045545639202080025\n",
      "Epoch 68/1000, Loss: 0.045545639202080025\n",
      "Epoch 70/1000, Loss: 0.045545639202080025\n",
      "Epoch 72/1000, Loss: 0.045545639202080025\n",
      "Epoch 74/1000, Loss: 0.045545639202080025\n",
      "Epoch 76/1000, Loss: 0.045545639202080025\n",
      "Epoch 78/1000, Loss: 0.045545639202080025\n",
      "Epoch 80/1000, Loss: 0.045545639202080025\n",
      "Epoch 82/1000, Loss: 0.045545639202080025\n",
      "Epoch 84/1000, Loss: 0.045545639202080025\n",
      "Epoch 86/1000, Loss: 0.045545639202080025\n",
      "Epoch 88/1000, Loss: 0.045545639202080025\n",
      "Epoch 90/1000, Loss: 0.045545639202080025\n",
      "Epoch 92/1000, Loss: 0.045545639202080025\n",
      "Epoch 94/1000, Loss: 0.045545639202080025\n",
      "Epoch 96/1000, Loss: 0.045545639202080025\n",
      "Epoch 98/1000, Loss: 0.045545639202080025\n",
      "Epoch 100/1000, Loss: 0.045545639202080025\n",
      "Epoch 102/1000, Loss: 0.045545639202080025\n",
      "Epoch 104/1000, Loss: 0.045545639202080025\n",
      "Epoch 106/1000, Loss: 0.045545639202080025\n",
      "Epoch 108/1000, Loss: 0.045545639202080025\n",
      "Epoch 110/1000, Loss: 0.045545639202080025\n",
      "Epoch 112/1000, Loss: 0.045545639202080025\n",
      "Epoch 114/1000, Loss: 0.045545639202080025\n",
      "Epoch 116/1000, Loss: 0.045545639202080025\n",
      "Epoch 118/1000, Loss: 0.045545639202080025\n",
      "Epoch 120/1000, Loss: 0.045545639202080025\n",
      "Epoch 122/1000, Loss: 0.045545639202080025\n",
      "Epoch 124/1000, Loss: 0.045545639202080025\n",
      "Epoch 126/1000, Loss: 0.045545639202080025\n",
      "Epoch 128/1000, Loss: 0.045545639202080025\n",
      "Epoch 130/1000, Loss: 0.045545639202080025\n",
      "Epoch 132/1000, Loss: 0.045545639202080025\n",
      "Epoch 134/1000, Loss: 0.045545639202080025\n",
      "Epoch 136/1000, Loss: 0.045545639202080025\n",
      "Epoch 138/1000, Loss: 0.045545639202080025\n",
      "Epoch 140/1000, Loss: 0.045545639202080025\n",
      "Epoch 142/1000, Loss: 0.045545639202080025\n",
      "Epoch 144/1000, Loss: 0.045545639202080025\n",
      "Epoch 146/1000, Loss: 0.045545639202080025\n",
      "Epoch 148/1000, Loss: 0.045545639202080025\n",
      "Epoch 150/1000, Loss: 0.045545639202080025\n",
      "Epoch 152/1000, Loss: 0.045545639202080025\n",
      "Epoch 154/1000, Loss: 0.045545639202080025\n",
      "Epoch 156/1000, Loss: 0.045545639202080025\n",
      "Epoch 158/1000, Loss: 0.045545639202080025\n",
      "Epoch 160/1000, Loss: 0.045545639202080025\n",
      "Epoch 162/1000, Loss: 0.045545639202080025\n",
      "Epoch 164/1000, Loss: 0.045545639202080025\n",
      "Epoch 166/1000, Loss: 0.045545639202080025\n",
      "Epoch 168/1000, Loss: 0.045545639202080025\n",
      "Epoch 170/1000, Loss: 0.045545639202080025\n",
      "Epoch 172/1000, Loss: 0.045545639202080025\n",
      "Epoch 174/1000, Loss: 0.045545639202080025\n",
      "Epoch 176/1000, Loss: 0.045545639202080025\n",
      "Epoch 178/1000, Loss: 0.045545639202080025\n",
      "Epoch 180/1000, Loss: 0.045545639202080025\n",
      "Epoch 182/1000, Loss: 0.045545639202080025\n",
      "Epoch 184/1000, Loss: 0.045545639202080025\n",
      "Epoch 186/1000, Loss: 0.045545639202080025\n",
      "Epoch 188/1000, Loss: 0.045545639202080025\n",
      "Epoch 190/1000, Loss: 0.045545639202080025\n",
      "Epoch 192/1000, Loss: 0.045545639202080025\n",
      "Epoch 194/1000, Loss: 0.045545639202080025\n",
      "Epoch 196/1000, Loss: 0.045545639202080025\n",
      "Epoch 198/1000, Loss: 0.045545639202080025\n",
      "Epoch 200/1000, Loss: 0.045545639202080025\n",
      "Epoch 202/1000, Loss: 0.045545639202080025\n",
      "Epoch 204/1000, Loss: 0.045545639202080025\n",
      "Epoch 206/1000, Loss: 0.045545639202080025\n",
      "Epoch 208/1000, Loss: 0.045545639202080025\n",
      "Epoch 210/1000, Loss: 0.045545639202080025\n",
      "Epoch 212/1000, Loss: 0.045545639202080025\n",
      "Epoch 214/1000, Loss: 0.045545639202080025\n",
      "Epoch 216/1000, Loss: 0.045545639202080025\n",
      "Epoch 218/1000, Loss: 0.045545639202080025\n",
      "Epoch 220/1000, Loss: 0.045545639202080025\n",
      "Epoch 222/1000, Loss: 0.045545639202080025\n",
      "Epoch 224/1000, Loss: 0.045545639202080025\n",
      "Epoch 226/1000, Loss: 0.045545639202080025\n",
      "Epoch 228/1000, Loss: 0.045545639202080025\n",
      "Epoch 230/1000, Loss: 0.045545639202080025\n",
      "Epoch 232/1000, Loss: 0.045545639202080025\n",
      "Epoch 234/1000, Loss: 0.045545639202080025\n",
      "Epoch 236/1000, Loss: 0.045545639202080025\n",
      "Epoch 238/1000, Loss: 0.045545639202080025\n",
      "Epoch 240/1000, Loss: 0.045545639202080025\n",
      "Epoch 242/1000, Loss: 0.045545639202080025\n",
      "Epoch 244/1000, Loss: 0.045545639202080025\n",
      "Epoch 246/1000, Loss: 0.045545639202080025\n",
      "Epoch 248/1000, Loss: 0.045545639202080025\n",
      "Epoch 250/1000, Loss: 0.045545639202080025\n",
      "Epoch 252/1000, Loss: 0.045545639202080025\n",
      "Epoch 254/1000, Loss: 0.045545639202080025\n",
      "Epoch 256/1000, Loss: 0.045545639202080025\n",
      "Epoch 258/1000, Loss: 0.045545639202080025\n",
      "Epoch 260/1000, Loss: 0.045545639202080025\n",
      "Epoch 262/1000, Loss: 0.045545639202080025\n",
      "Epoch 264/1000, Loss: 0.045545639202080025\n",
      "Epoch 266/1000, Loss: 0.045545639202080025\n",
      "Epoch 268/1000, Loss: 0.045545639202080025\n",
      "Epoch 270/1000, Loss: 0.045545639202080025\n",
      "Epoch 272/1000, Loss: 0.045545639202080025\n",
      "Epoch 274/1000, Loss: 0.045545639202080025\n",
      "Epoch 276/1000, Loss: 0.045545639202080025\n",
      "Epoch 278/1000, Loss: 0.045545639202080025\n",
      "Epoch 280/1000, Loss: 0.045545639202080025\n",
      "Epoch 282/1000, Loss: 0.045545639202080025\n",
      "Epoch 284/1000, Loss: 0.045545639202080025\n",
      "Epoch 286/1000, Loss: 0.045545639202080025\n",
      "Epoch 288/1000, Loss: 0.045545639202080025\n",
      "Epoch 290/1000, Loss: 0.045545639202080025\n",
      "Epoch 292/1000, Loss: 0.045545639202080025\n",
      "Epoch 294/1000, Loss: 0.045545639202080025\n",
      "Epoch 296/1000, Loss: 0.045545639202080025\n",
      "Epoch 298/1000, Loss: 0.045545639202080025\n",
      "Epoch 300/1000, Loss: 0.045545639202080025\n",
      "Epoch 302/1000, Loss: 0.045545639202080025\n",
      "Epoch 304/1000, Loss: 0.045545639202080025\n",
      "Epoch 306/1000, Loss: 0.045545639202080025\n",
      "Epoch 308/1000, Loss: 0.045545639202080025\n",
      "Epoch 310/1000, Loss: 0.045545639202080025\n",
      "Epoch 312/1000, Loss: 0.045545639202080025\n",
      "Epoch 314/1000, Loss: 0.045545639202080025\n",
      "Epoch 316/1000, Loss: 0.045545639202080025\n",
      "Epoch 318/1000, Loss: 0.045545639202080025\n",
      "Epoch 320/1000, Loss: 0.045545639202080025\n",
      "Epoch 322/1000, Loss: 0.045545639202080025\n",
      "Epoch 324/1000, Loss: 0.045545639202080025\n",
      "Epoch 326/1000, Loss: 0.045545639202080025\n",
      "Epoch 328/1000, Loss: 0.045545639202080025\n",
      "Epoch 330/1000, Loss: 0.045545639202080025\n",
      "Epoch 332/1000, Loss: 0.045545639202080025\n",
      "Epoch 334/1000, Loss: 0.045545639202080025\n",
      "Epoch 336/1000, Loss: 0.045545639202080025\n",
      "Epoch 338/1000, Loss: 0.045545639202080025\n",
      "Epoch 340/1000, Loss: 0.045545639202080025\n",
      "Epoch 342/1000, Loss: 0.045545639202080025\n",
      "Epoch 344/1000, Loss: 0.045545639202080025\n",
      "Epoch 346/1000, Loss: 0.045545639202080025\n",
      "Epoch 348/1000, Loss: 0.045545639202080025\n",
      "Epoch 350/1000, Loss: 0.045545639202080025\n",
      "Epoch 352/1000, Loss: 0.045545639202080025\n",
      "Epoch 354/1000, Loss: 0.045545639202080025\n",
      "Epoch 356/1000, Loss: 0.045545639202080025\n",
      "Epoch 358/1000, Loss: 0.045545639202080025\n",
      "Epoch 360/1000, Loss: 0.045545639202080025\n",
      "Epoch 362/1000, Loss: 0.045545639202080025\n",
      "Epoch 364/1000, Loss: 0.045545639202080025\n",
      "Epoch 366/1000, Loss: 0.045545639202080025\n",
      "Epoch 368/1000, Loss: 0.045545639202080025\n",
      "Epoch 370/1000, Loss: 0.045545639202080025\n",
      "Epoch 372/1000, Loss: 0.045545639202080025\n",
      "Epoch 374/1000, Loss: 0.045545639202080025\n",
      "Epoch 376/1000, Loss: 0.045545639202080025\n",
      "Epoch 378/1000, Loss: 0.045545639202080025\n",
      "Epoch 380/1000, Loss: 0.045545639202080025\n",
      "Epoch 382/1000, Loss: 0.045545639202080025\n",
      "Epoch 384/1000, Loss: 0.045545639202080025\n",
      "Epoch 386/1000, Loss: 0.045545639202080025\n",
      "Epoch 388/1000, Loss: 0.045545639202080025\n",
      "Epoch 390/1000, Loss: 0.045545639202080025\n",
      "Epoch 392/1000, Loss: 0.045545639202080025\n",
      "Epoch 394/1000, Loss: 0.045545639202080025\n",
      "Epoch 396/1000, Loss: 0.045545639202080025\n",
      "Epoch 398/1000, Loss: 0.045545639202080025\n",
      "Epoch 400/1000, Loss: 0.045545639202080025\n",
      "Epoch 402/1000, Loss: 0.045545639202080025\n",
      "Epoch 404/1000, Loss: 0.045545639202080025\n",
      "Epoch 406/1000, Loss: 0.045545639202080025\n",
      "Epoch 408/1000, Loss: 0.045545639202080025\n",
      "Epoch 410/1000, Loss: 0.045545639202080025\n",
      "Epoch 412/1000, Loss: 0.045545639202080025\n",
      "Epoch 414/1000, Loss: 0.045545639202080025\n",
      "Epoch 416/1000, Loss: 0.045545639202080025\n",
      "Epoch 418/1000, Loss: 0.045545639202080025\n",
      "Epoch 420/1000, Loss: 0.045545639202080025\n",
      "Epoch 422/1000, Loss: 0.045545639202080025\n",
      "Epoch 424/1000, Loss: 0.045545639202080025\n",
      "Epoch 426/1000, Loss: 0.045545639202080025\n",
      "Epoch 428/1000, Loss: 0.045545639202080025\n",
      "Epoch 430/1000, Loss: 0.045545639202080025\n",
      "Epoch 432/1000, Loss: 0.045545639202080025\n",
      "Epoch 434/1000, Loss: 0.045545639202080025\n",
      "Epoch 436/1000, Loss: 0.045545639202080025\n",
      "Epoch 438/1000, Loss: 0.045545639202080025\n",
      "Epoch 440/1000, Loss: 0.045545639202080025\n",
      "Epoch 442/1000, Loss: 0.045545639202080025\n",
      "Epoch 444/1000, Loss: 0.045545639202080025\n",
      "Epoch 446/1000, Loss: 0.045545639202080025\n",
      "Epoch 448/1000, Loss: 0.045545639202080025\n",
      "Epoch 450/1000, Loss: 0.045545639202080025\n",
      "Epoch 452/1000, Loss: 0.045545639202080025\n",
      "Epoch 454/1000, Loss: 0.045545639202080025\n",
      "Epoch 456/1000, Loss: 0.045545639202080025\n",
      "Epoch 458/1000, Loss: 0.045545639202080025\n",
      "Epoch 460/1000, Loss: 0.045545639202080025\n",
      "Epoch 462/1000, Loss: 0.045545639202080025\n",
      "Epoch 464/1000, Loss: 0.045545639202080025\n",
      "Epoch 466/1000, Loss: 0.045545639202080025\n",
      "Epoch 468/1000, Loss: 0.045545639202080025\n",
      "Epoch 470/1000, Loss: 0.045545639202080025\n",
      "Epoch 472/1000, Loss: 0.045545639202080025\n",
      "Epoch 474/1000, Loss: 0.045545639202080025\n",
      "Epoch 476/1000, Loss: 0.045545639202080025\n",
      "Epoch 478/1000, Loss: 0.045545639202080025\n",
      "Epoch 480/1000, Loss: 0.045545639202080025\n",
      "Epoch 482/1000, Loss: 0.045545639202080025\n",
      "Epoch 484/1000, Loss: 0.045545639202080025\n",
      "Epoch 486/1000, Loss: 0.045545639202080025\n",
      "Epoch 488/1000, Loss: 0.045545639202080025\n",
      "Epoch 490/1000, Loss: 0.045545639202080025\n",
      "Epoch 492/1000, Loss: 0.045545639202080025\n",
      "Epoch 494/1000, Loss: 0.045545639202080025\n",
      "Epoch 496/1000, Loss: 0.045545639202080025\n",
      "Epoch 498/1000, Loss: 0.045545639202080025\n",
      "Epoch 500/1000, Loss: 0.045545639202080025\n",
      "Epoch 502/1000, Loss: 0.045545639202080025\n",
      "Epoch 504/1000, Loss: 0.045545639202080025\n",
      "Epoch 506/1000, Loss: 0.045545639202080025\n",
      "Epoch 508/1000, Loss: 0.045545639202080025\n",
      "Epoch 510/1000, Loss: 0.045545639202080025\n",
      "Epoch 512/1000, Loss: 0.045545639202080025\n",
      "Epoch 514/1000, Loss: 0.045545639202080025\n",
      "Epoch 516/1000, Loss: 0.045545639202080025\n",
      "Epoch 518/1000, Loss: 0.045545639202080025\n",
      "Epoch 520/1000, Loss: 0.045545639202080025\n",
      "Epoch 522/1000, Loss: 0.045545639202080025\n",
      "Epoch 524/1000, Loss: 0.045545639202080025\n",
      "Epoch 526/1000, Loss: 0.045545639202080025\n",
      "Epoch 528/1000, Loss: 0.045545639202080025\n",
      "Epoch 530/1000, Loss: 0.045545639202080025\n",
      "Epoch 532/1000, Loss: 0.045545639202080025\n",
      "Epoch 534/1000, Loss: 0.045545639202080025\n",
      "Epoch 536/1000, Loss: 0.045545639202080025\n",
      "Epoch 538/1000, Loss: 0.045545639202080025\n",
      "Epoch 540/1000, Loss: 0.045545639202080025\n",
      "Epoch 542/1000, Loss: 0.045545639202080025\n",
      "Epoch 544/1000, Loss: 0.045545639202080025\n",
      "Epoch 546/1000, Loss: 0.045545639202080025\n",
      "Epoch 548/1000, Loss: 0.045545639202080025\n",
      "Epoch 550/1000, Loss: 0.045545639202080025\n",
      "Epoch 552/1000, Loss: 0.045545639202080025\n",
      "Epoch 554/1000, Loss: 0.045545639202080025\n",
      "Epoch 556/1000, Loss: 0.045545639202080025\n",
      "Epoch 558/1000, Loss: 0.045545639202080025\n",
      "Epoch 560/1000, Loss: 0.045545639202080025\n",
      "Epoch 562/1000, Loss: 0.045545639202080025\n",
      "Epoch 564/1000, Loss: 0.045545639202080025\n",
      "Epoch 566/1000, Loss: 0.045545639202080025\n",
      "Epoch 568/1000, Loss: 0.045545639202080025\n",
      "Epoch 570/1000, Loss: 0.045545639202080025\n",
      "Epoch 572/1000, Loss: 0.045545639202080025\n",
      "Epoch 574/1000, Loss: 0.045545639202080025\n",
      "Epoch 576/1000, Loss: 0.045545639202080025\n",
      "Epoch 578/1000, Loss: 0.045545639202080025\n",
      "Epoch 580/1000, Loss: 0.045545639202080025\n",
      "Epoch 582/1000, Loss: 0.045545639202080025\n",
      "Epoch 584/1000, Loss: 0.045545639202080025\n",
      "Epoch 586/1000, Loss: 0.045545639202080025\n",
      "Epoch 588/1000, Loss: 0.045545639202080025\n",
      "Epoch 590/1000, Loss: 0.045545639202080025\n",
      "Epoch 592/1000, Loss: 0.045545639202080025\n",
      "Epoch 594/1000, Loss: 0.045545639202080025\n",
      "Epoch 596/1000, Loss: 0.045545639202080025\n",
      "Epoch 598/1000, Loss: 0.045545639202080025\n",
      "Epoch 600/1000, Loss: 0.045545639202080025\n",
      "Epoch 602/1000, Loss: 0.045545639202080025\n",
      "Epoch 604/1000, Loss: 0.045545639202080025\n",
      "Epoch 606/1000, Loss: 0.045545639202080025\n",
      "Epoch 608/1000, Loss: 0.045545639202080025\n",
      "Epoch 610/1000, Loss: 0.045545639202080025\n",
      "Epoch 612/1000, Loss: 0.045545639202080025\n",
      "Epoch 614/1000, Loss: 0.045545639202080025\n",
      "Epoch 616/1000, Loss: 0.045545639202080025\n",
      "Epoch 618/1000, Loss: 0.045545639202080025\n",
      "Epoch 620/1000, Loss: 0.045545639202080025\n",
      "Epoch 622/1000, Loss: 0.045545639202080025\n",
      "Epoch 624/1000, Loss: 0.045545639202080025\n",
      "Epoch 626/1000, Loss: 0.045545639202080025\n",
      "Epoch 628/1000, Loss: 0.045545639202080025\n",
      "Epoch 630/1000, Loss: 0.045545639202080025\n",
      "Epoch 632/1000, Loss: 0.045545639202080025\n",
      "Epoch 634/1000, Loss: 0.045545639202080025\n",
      "Epoch 636/1000, Loss: 0.045545639202080025\n",
      "Epoch 638/1000, Loss: 0.045545639202080025\n",
      "Epoch 640/1000, Loss: 0.045545639202080025\n",
      "Epoch 642/1000, Loss: 0.045545639202080025\n",
      "Epoch 644/1000, Loss: 0.045545639202080025\n",
      "Epoch 646/1000, Loss: 0.045545639202080025\n",
      "Epoch 648/1000, Loss: 0.045545639202080025\n",
      "Epoch 650/1000, Loss: 0.045545639202080025\n",
      "Epoch 652/1000, Loss: 0.045545639202080025\n",
      "Epoch 654/1000, Loss: 0.045545639202080025\n",
      "Epoch 656/1000, Loss: 0.045545639202080025\n",
      "Epoch 658/1000, Loss: 0.045545639202080025\n",
      "Epoch 660/1000, Loss: 0.045545639202080025\n",
      "Epoch 662/1000, Loss: 0.045545639202080025\n",
      "Epoch 664/1000, Loss: 0.045545639202080025\n",
      "Epoch 666/1000, Loss: 0.045545639202080025\n",
      "Epoch 668/1000, Loss: 0.045545639202080025\n",
      "Epoch 670/1000, Loss: 0.045545639202080025\n",
      "Epoch 672/1000, Loss: 0.045545639202080025\n",
      "Epoch 674/1000, Loss: 0.045545639202080025\n",
      "Epoch 676/1000, Loss: 0.045545639202080025\n",
      "Epoch 678/1000, Loss: 0.045545639202080025\n",
      "Epoch 680/1000, Loss: 0.045545639202080025\n",
      "Epoch 682/1000, Loss: 0.045545639202080025\n",
      "Epoch 684/1000, Loss: 0.045545639202080025\n",
      "Epoch 686/1000, Loss: 0.045545639202080025\n",
      "Epoch 688/1000, Loss: 0.045545639202080025\n",
      "Epoch 690/1000, Loss: 0.045545639202080025\n",
      "Epoch 692/1000, Loss: 0.045545639202080025\n",
      "Epoch 694/1000, Loss: 0.045545639202080025\n",
      "Epoch 696/1000, Loss: 0.045545639202080025\n",
      "Epoch 698/1000, Loss: 0.045545639202080025\n",
      "Epoch 700/1000, Loss: 0.045545639202080025\n",
      "Epoch 702/1000, Loss: 0.045545639202080025\n",
      "Epoch 704/1000, Loss: 0.045545639202080025\n",
      "Epoch 706/1000, Loss: 0.045545639202080025\n",
      "Epoch 708/1000, Loss: 0.045545639202080025\n",
      "Epoch 710/1000, Loss: 0.045545639202080025\n",
      "Epoch 712/1000, Loss: 0.045545639202080025\n",
      "Epoch 714/1000, Loss: 0.045545639202080025\n",
      "Epoch 716/1000, Loss: 0.045545639202080025\n",
      "Epoch 718/1000, Loss: 0.045545639202080025\n",
      "Epoch 720/1000, Loss: 0.045545639202080025\n",
      "Epoch 722/1000, Loss: 0.045545639202080025\n",
      "Epoch 724/1000, Loss: 0.045545639202080025\n",
      "Epoch 726/1000, Loss: 0.045545639202080025\n",
      "Epoch 728/1000, Loss: 0.045545639202080025\n",
      "Epoch 730/1000, Loss: 0.045545639202080025\n",
      "Epoch 732/1000, Loss: 0.045545639202080025\n",
      "Epoch 734/1000, Loss: 0.045545639202080025\n",
      "Epoch 736/1000, Loss: 0.045545639202080025\n",
      "Epoch 738/1000, Loss: 0.045545639202080025\n",
      "Epoch 740/1000, Loss: 0.045545639202080025\n",
      "Epoch 742/1000, Loss: 0.045545639202080025\n",
      "Epoch 744/1000, Loss: 0.045545639202080025\n",
      "Epoch 746/1000, Loss: 0.045545639202080025\n",
      "Epoch 748/1000, Loss: 0.045545639202080025\n",
      "Epoch 750/1000, Loss: 0.045545639202080025\n",
      "Epoch 752/1000, Loss: 0.045545639202080025\n",
      "Epoch 754/1000, Loss: 0.045545639202080025\n",
      "Epoch 756/1000, Loss: 0.045545639202080025\n",
      "Epoch 758/1000, Loss: 0.045545639202080025\n",
      "Epoch 760/1000, Loss: 0.045545639202080025\n",
      "Epoch 762/1000, Loss: 0.045545639202080025\n",
      "Epoch 764/1000, Loss: 0.045545639202080025\n",
      "Epoch 766/1000, Loss: 0.045545639202080025\n",
      "Epoch 768/1000, Loss: 0.045545639202080025\n",
      "Epoch 770/1000, Loss: 0.045545639202080025\n",
      "Epoch 772/1000, Loss: 0.045545639202080025\n",
      "Epoch 774/1000, Loss: 0.045545639202080025\n",
      "Epoch 776/1000, Loss: 0.045545639202080025\n",
      "Epoch 778/1000, Loss: 0.045545639202080025\n",
      "Epoch 780/1000, Loss: 0.045545639202080025\n",
      "Epoch 782/1000, Loss: 0.045545639202080025\n",
      "Epoch 784/1000, Loss: 0.045545639202080025\n",
      "Epoch 786/1000, Loss: 0.045545639202080025\n",
      "Epoch 788/1000, Loss: 0.045545639202080025\n",
      "Epoch 790/1000, Loss: 0.045545639202080025\n",
      "Epoch 792/1000, Loss: 0.045545639202080025\n",
      "Epoch 794/1000, Loss: 0.045545639202080025\n",
      "Epoch 796/1000, Loss: 0.045545639202080025\n",
      "Epoch 798/1000, Loss: 0.045545639202080025\n",
      "Epoch 800/1000, Loss: 0.045545639202080025\n",
      "Epoch 802/1000, Loss: 0.045545639202080025\n",
      "Epoch 804/1000, Loss: 0.045545639202080025\n",
      "Epoch 806/1000, Loss: 0.045545639202080025\n",
      "Epoch 808/1000, Loss: 0.045545639202080025\n",
      "Epoch 810/1000, Loss: 0.045545639202080025\n",
      "Epoch 812/1000, Loss: 0.045545639202080025\n",
      "Epoch 814/1000, Loss: 0.045545639202080025\n",
      "Epoch 816/1000, Loss: 0.045545639202080025\n",
      "Epoch 818/1000, Loss: 0.045545639202080025\n",
      "Epoch 820/1000, Loss: 0.045545639202080025\n",
      "Epoch 822/1000, Loss: 0.045545639202080025\n",
      "Epoch 824/1000, Loss: 0.045545639202080025\n",
      "Epoch 826/1000, Loss: 0.045545639202080025\n",
      "Epoch 828/1000, Loss: 0.045545639202080025\n",
      "Epoch 830/1000, Loss: 0.045545639202080025\n",
      "Epoch 832/1000, Loss: 0.045545639202080025\n",
      "Epoch 834/1000, Loss: 0.045545639202080025\n",
      "Epoch 836/1000, Loss: 0.045545639202080025\n",
      "Epoch 838/1000, Loss: 0.045545639202080025\n",
      "Epoch 840/1000, Loss: 0.045545639202080025\n",
      "Epoch 842/1000, Loss: 0.045545639202080025\n",
      "Epoch 844/1000, Loss: 0.045545639202080025\n",
      "Epoch 846/1000, Loss: 0.045545639202080025\n",
      "Epoch 848/1000, Loss: 0.045545639202080025\n",
      "Epoch 850/1000, Loss: 0.045545639202080025\n",
      "Epoch 852/1000, Loss: 0.045545639202080025\n",
      "Epoch 854/1000, Loss: 0.045545639202080025\n",
      "Epoch 856/1000, Loss: 0.045545639202080025\n",
      "Epoch 858/1000, Loss: 0.045545639202080025\n",
      "Epoch 860/1000, Loss: 0.045545639202080025\n",
      "Epoch 862/1000, Loss: 0.045545639202080025\n",
      "Epoch 864/1000, Loss: 0.045545639202080025\n",
      "Epoch 866/1000, Loss: 0.045545639202080025\n",
      "Epoch 868/1000, Loss: 0.045545639202080025\n",
      "Epoch 870/1000, Loss: 0.045545639202080025\n",
      "Epoch 872/1000, Loss: 0.045545639202080025\n",
      "Epoch 874/1000, Loss: 0.045545639202080025\n",
      "Epoch 876/1000, Loss: 0.045545639202080025\n",
      "Epoch 878/1000, Loss: 0.045545639202080025\n",
      "Epoch 880/1000, Loss: 0.045545639202080025\n",
      "Epoch 882/1000, Loss: 0.045545639202080025\n",
      "Epoch 884/1000, Loss: 0.045545639202080025\n",
      "Epoch 886/1000, Loss: 0.045545639202080025\n",
      "Epoch 888/1000, Loss: 0.045545639202080025\n",
      "Epoch 890/1000, Loss: 0.045545639202080025\n",
      "Epoch 892/1000, Loss: 0.045545639202080025\n",
      "Epoch 894/1000, Loss: 0.045545639202080025\n",
      "Epoch 896/1000, Loss: 0.045545639202080025\n",
      "Epoch 898/1000, Loss: 0.045545639202080025\n",
      "Epoch 900/1000, Loss: 0.045545639202080025\n",
      "Epoch 902/1000, Loss: 0.045545639202080025\n",
      "Epoch 904/1000, Loss: 0.045545639202080025\n",
      "Epoch 906/1000, Loss: 0.045545639202080025\n",
      "Epoch 908/1000, Loss: 0.045545639202080025\n",
      "Epoch 910/1000, Loss: 0.045545639202080025\n",
      "Epoch 912/1000, Loss: 0.045545639202080025\n",
      "Epoch 914/1000, Loss: 0.045545639202080025\n",
      "Epoch 916/1000, Loss: 0.045545639202080025\n",
      "Epoch 918/1000, Loss: 0.045545639202080025\n",
      "Epoch 920/1000, Loss: 0.045545639202080025\n",
      "Epoch 922/1000, Loss: 0.045545639202080025\n",
      "Epoch 924/1000, Loss: 0.045545639202080025\n",
      "Epoch 926/1000, Loss: 0.045545639202080025\n",
      "Epoch 928/1000, Loss: 0.045545639202080025\n",
      "Epoch 930/1000, Loss: 0.045545639202080025\n",
      "Epoch 932/1000, Loss: 0.045545639202080025\n",
      "Epoch 934/1000, Loss: 0.045545639202080025\n",
      "Epoch 936/1000, Loss: 0.045545639202080025\n",
      "Epoch 938/1000, Loss: 0.045545639202080025\n",
      "Epoch 940/1000, Loss: 0.045545639202080025\n",
      "Epoch 942/1000, Loss: 0.045545639202080025\n",
      "Epoch 944/1000, Loss: 0.045545639202080025\n",
      "Epoch 946/1000, Loss: 0.045545639202080025\n",
      "Epoch 948/1000, Loss: 0.045545639202080025\n",
      "Epoch 950/1000, Loss: 0.045545639202080025\n",
      "Epoch 952/1000, Loss: 0.045545639202080025\n",
      "Epoch 954/1000, Loss: 0.045545639202080025\n",
      "Epoch 956/1000, Loss: 0.045545639202080025\n",
      "Epoch 958/1000, Loss: 0.045545639202080025\n",
      "Epoch 960/1000, Loss: 0.045545639202080025\n",
      "Epoch 962/1000, Loss: 0.045545639202080025\n",
      "Epoch 964/1000, Loss: 0.045545639202080025\n",
      "Epoch 966/1000, Loss: 0.045545639202080025\n",
      "Epoch 968/1000, Loss: 0.045545639202080025\n",
      "Epoch 970/1000, Loss: 0.045545639202080025\n",
      "Epoch 972/1000, Loss: 0.045545639202080025\n",
      "Epoch 974/1000, Loss: 0.045545639202080025\n",
      "Epoch 976/1000, Loss: 0.045545639202080025\n",
      "Epoch 978/1000, Loss: 0.045545639202080025\n",
      "Epoch 980/1000, Loss: 0.045545639202080025\n",
      "Epoch 982/1000, Loss: 0.045545639202080025\n",
      "Epoch 984/1000, Loss: 0.045545639202080025\n",
      "Epoch 986/1000, Loss: 0.045545639202080025\n",
      "Epoch 988/1000, Loss: 0.045545639202080025\n",
      "Epoch 990/1000, Loss: 0.045545639202080025\n",
      "Epoch 992/1000, Loss: 0.045545639202080025\n",
      "Epoch 994/1000, Loss: 0.045545639202080025\n",
      "Epoch 996/1000, Loss: 0.045545639202080025\n",
      "Epoch 998/1000, Loss: 0.045545639202080025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8b/68ndjfsd6113_1n6vlxn4_qw0000gn/T/ipykernel_48199/4123805106.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "model = logisticRegression(0.01,1000)\n",
    "weights, bias, loss = model.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8b/68ndjfsd6113_1n6vlxn4_qw0000gn/T/ipykernel_48199/4123805106.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "**Why use a different loss function from linear regression?**\n",
    "\n",
    "If you try to use the linear regression's cost function in a logistic regression problem, you would end up with a non-convex function: a wierdly-shaped graph with no easy to find minimum global point. \n",
    "\n",
    "This strange outcome is due to the fact that in logistic regression we have the sigmoid function around, which is non-linear (i.e. not a line). The gradient descent algorithm might get stuck in a local minimum point. That's why we still need a neat convex function as we did for linear regression: a bowl-shaped function that eases the gradient descent function's work to converge to the optimal minimum point.\n",
    "\n",
    "*Source can be found [here](https://www.internalpointers.com/post/cost-function-logistic-regression)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
